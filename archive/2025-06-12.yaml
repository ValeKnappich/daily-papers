date: '2025-06-12'
papers:
- title: "Obliviate: Efficient Unmemorization for Protecting Intellectual Property\n\
    \  in Large Language Models"
  authors:
  - Mark Russinovich
  - Ahmed Salem
  summary: Recent copyright agreements between AI companies and content creators underscore
    the need for fine-grained control over language models' ability to reproduce copyrighted
    text. Existing defenses-ranging from aggressive unlearning to simplistic output
    filters-either sacrifice model utility or inadequately address verbatim leakage.
    We introduce Obliviate, a lightweight post-training method that surgically suppresses
    exact reproduction of specified sequences while preserving semantic understanding.
    Obliviate first identifies memorized passages and then, for each target token,
    minimally adjusts the model's output distribution via a Kullback-Leibler divergence
    penalty to drive down the probability of exact reproduction. Simultaneously, we
    enforce a consistency loss on non-target tokens to retain the model's fluency
    and task performance. We evaluate Obliviate on four popular 6-8B-parameter models
    (LLaMA-3.1, LLaMA-3.1-Instruct, Qwen-2.5, and Yi-1.5) using synthetic memorization
    benchmarks and organic copyrighted excerpts (e.g., Moby Dick, Frankenstein, Alice
    in Wonderland and Les Miserables). Across all settings, Obliviate reduces verbatim
    recall by two orders of magnitude (e.g., from hundreds of words to fewer than
    12) while degrading downstream accuracy by at most 1% on HellaSwag, MMLU, TruthfulQA,
    and Winogrande. Furthermore, we benchmark Obliviate aganist different unlearning
    and copyright techniques using the MUSE and CoTaEval benchmarks. These results
    position Obliviate as a practical, high-fidelity solution for copyright compliance
    in deployed LLMs.
  link: http://arxiv.org/abs/2502.15010v2
  published: '2025-02-20T20:02:56Z'
