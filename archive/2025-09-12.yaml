date: '2025-09-12'
papers:
- title: "Can Large Language Models Understand As Well As Apply Patent Regulations\n\
    \  to Pass a Hands-On Patent Attorney Test?"
  authors:
  - Bhakti Khera
  - Rezvan Alamian
  - Pascal A. Scherz
  - Stephan M. Goetz
  summary: The legal field already uses various large language models (LLMs) in actual
    applications, but their quantitative performance and reasons for it are underexplored.
    We evaluated several open-source and proprietary LLMs -- including GPT-series,
    Anthropic, Deepseek and Llama-3, variants -- on parts of the European Qualifying
    Examination (EQE) for future European Patent Attorneys. OpenAI o1 led with 0.82
    accuracy and 0.81 F1 score, whereas (Amazon Web Services) AWS Llama 3.1 8B lagged
    at 0.50 accuracy, and a Python-deployed Llama 3.1 8B scored 0.55. The latter two
    are within the range of mere guessing for the two-answer forced-choice design.
    None of the evaluated models could have passed the examination fully, as accuracy
    never exceeded the average threshold of 0.90 required for professional-level standards
    -- also not models that are regularly promoted for their assumed beyond-PhD- and
    bar-admitted-lawyer-level performance. GPT-4o excelled at integrating text and
    graphics, while Claude 3 Opus often lost formatting coherence. Human patent experts
    evaluated the textual justifications and uncovered various critical shortcomings
    of each model. They valued clarity and legal rationale over the raw correctness
    of the answers, which revealed misalignment between automatic metrics and expert
    judgment. Model outputs were sensitive to modest temperature changes and prompt
    wording, which underscores the remaining necessity of expert oversight. Future
    work should target logical consistency, robust multimodality, and adaptive prompting
    to approach human-level patent proficiency. In summary, despite the outstanding
    performance of recent large models, the general public might overestimate their
    performance. The field has a long way to go to develop a virtual patent attorney.
    This paper wants to point out several specific limitations that need solutions.
  link: http://arxiv.org/abs/2507.10576v2
  published: '2025-07-11T09:42:23Z'
- title: "From scratch to silver: Creating trustworthy training data for\n  patent-SDG\
    \ classification using Large Language Models"
  authors:
  - Grazia Sveva Ascione
  - Nicol√≤ Tamagnone
  summary: 'Classifying patents by their relevance to the UN Sustainable Development
    Goals (SDGs) is crucial for tracking how innovation addresses global challenges.
    However, the absence of a large, labeled dataset limits the use of supervised
    learning. Existing methods, such as keyword searches, transfer learning, and citation-based
    heuristics, lack scalability and generalizability. This paper frames patent-to-SDG
    classification as a weak supervision problem, using citations from patents to
    SDG-tagged scientific publications (NPL citations) as a noisy initial signal.
    To address its sparsity and noise, we develop a composite labeling function (LF)
    that uses large language models (LLMs) to extract structured concepts, namely
    functions, solutions, and applications, from patents and SDG papers based on a
    patent ontology. Cross-domain similarity scores are computed and combined using
    a rank-based retrieval approach. The LF is calibrated via a custom positive-only
    loss that aligns with known NPL-SDG links without penalizing discovery of new
    SDG associations. The result is a silver-standard, soft multi-label dataset mapping
    patents to SDGs, enabling the training of effective multi-label regression models.
    We validate our approach through two complementary strategies: (1) internal validation
    against held-out NPL-based labels, where our method outperforms several baselines
    including transformer-based models, and zero-shot LLM; and (2) external validation
    using network modularity in patent citation, co-inventor, and co-applicant graphs,
    where our labels reveal greater thematic, cognitive, and organizational coherence
    than traditional technological classifications. These results show that weak supervision
    and semantic alignment can enhance SDG classification at scale.'
  link: http://arxiv.org/abs/2509.09303v1
  published: '2025-09-11T09:44:16Z'
