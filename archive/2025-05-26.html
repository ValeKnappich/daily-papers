<!DOCTYPE html><!--__xLLSGpKpbeK61bOsBjx--><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/daily-papers/_next/static/css/a760a3164bba60fd.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/daily-papers/_next/static/chunks/webpack-fc8e233cfa9e6f69.js"/><script src="/daily-papers/_next/static/chunks/4bd1b696-c49c6f05a40ba469.js" async=""></script><script src="/daily-papers/_next/static/chunks/964-dff8478de30a6510.js" async=""></script><script src="/daily-papers/_next/static/chunks/main-app-00615a32e0ffc0ce.js" async=""></script><script src="/daily-papers/_next/static/chunks/874-437a265a67d6cfee.js" async=""></script><script src="/daily-papers/_next/static/chunks/app/archive/%5Bdate%5D/page-a387a16caeea677c.js" async=""></script><script src="/daily-papers/_next/static/chunks/63-6e23fca05d5bdafc.js" async=""></script><script src="/daily-papers/_next/static/chunks/app/layout-9d71c8e95472ad59.js" async=""></script><title>Daily Papers</title><meta name="description" content="Stay updated with the latest research papers"/><link rel="icon" href="/daily-papers/favicon.ico" type="image/x-icon" sizes="1024x1024"/><script src="/daily-papers/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="bg-zinc-50 dark:bg-zinc-950 text-zinc-900 dark:text-zinc-100 min-h-screen"><div hidden=""><!--$--><!--/$--></div><header class="w-full py-4 border-b border-zinc-200 dark:border-zinc-800 mb-8"><div class="max-w-6xl mx-auto px-4 flex items-center justify-between"><a class="flex items-center gap-3 hover:opacity-80 transition-opacity" href="/daily-papers"><img alt="favicon" loading="lazy" width="28" height="28" decoding="async" data-nimg="1" class="rounded" style="color:transparent" src="/daily-papers/favicon.png"/><span class="font-bold text-lg tracking-tight">Daily Papers</span></a><div class="flex items-center gap-4"><div class="relative"><button class="px-3 py-1 rounded bg-zinc-100 dark:bg-zinc-800 border border-zinc-300 dark:border-zinc-700 text-sm font-medium hover:bg-zinc-200 dark:hover:bg-zinc-700 focus:outline-none" aria-haspopup="listbox" aria-expanded="false">Past Days<span class="ml-2">▼</span></button></div></div></div></header><main><main class="max-w-6xl mx-auto py-10 px-4"><h1 class="text-2xl font-extrabold mb-8 tracking-tight text-center">Papers for <!-- -->2025-05-26</h1><div class="prose dark:prose-invert border rounded p-4 bg-white dark:bg-zinc-900 mb-8"><div class="mb-8"><h2 class="text-xl font-semibold mb-1">PatentMind: A Multi-Aspect Reasoning Graph for Patent Similarity
  Evaluation</h2><div class="text-sm text-gray-600 mb-1"><b>Authors:</b> <!-- -->Yongmin Yoo, Qiongkai Xu, Longbing Cao</div><div class="text-sm text-gray-600 mb-1"><b>Published:</b> <!-- -->2025-05-25T22:28:27Z</div><div class="text-sm text-gray-600 mb-2"><b>Link:</b> <a href="http://arxiv.org/abs/2505.19347v1" class="text-blue-600 hover:underline" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2505.19347v1</a></div><div class="whitespace-pre-line">Patent similarity evaluation plays a critical role in intellectual property analysis. However, existing methods often overlook the intricate structure of patent documents, which integrate technical specifications, legal boundaries, and application contexts. We introduce PatentMind, a novel framework for patent similarity assessment based on a Multi-Aspect Reasoning Graph (MARG). PatentMind decomposes patents into three core dimensions: technical feature, application domain, and claim scope, to compute dimension-specific similarity scores. These scores are dynamically weighted through a four-stage reasoning process which integrates contextual signals to emulate expert-level judgment. To support evaluation, we construct PatentSimBench, a human-annotated benchmark comprising 500 patent pairs. Experimental results demonstrate that PatentMind achieves a strong correlation ($r=0.938$) with expert annotations, significantly outperforming embedding-based models and advanced prompt engineering methods.These results highlight the effectiveness of modular reasoning frameworks in overcoming key limitations of embedding-based methods for analyzing patent similarity.</div></div><div class="mb-8"><h2 class="text-xl font-semibold mb-1">PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims</h2><div class="text-sm text-gray-600 mb-1"><b>Authors:</b> <!-- -->Yongmin Yoo, Qiongkai Xu, Longbing Cao</div><div class="text-sm text-gray-600 mb-1"><b>Published:</b> <!-- -->2025-05-25T22:20:11Z</div><div class="text-sm text-gray-600 mb-2"><b>Link:</b> <a href="http://arxiv.org/abs/2505.19345v1" class="text-blue-600 hover:underline" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2505.19345v1</a></div><div class="whitespace-pre-line">Natural language generation (NLG) metrics play a central role in evaluating generated texts, but are not well suited for the structural and legal characteristics of patent documents. Large language models (LLMs) offer strong potential in automating patent generation, yet research on evaluating LLM-generated patents remains limited, especially in evaluating the generation quality of patent claims, which are central to defining the scope of protection. Effective claim evaluation requires addressing legal validity, technical accuracy, and structural compliance. To address this gap, we introduce PatentScore, a multi-dimensional evaluation framework for assessing LLM-generated patent claims. PatentScore incorporates: (1) hierarchical decomposition for claim analysis; (2) domain-specific validation patterns based on legal and technical standards; and (3) scoring across structural, semantic, and legal dimensions. Unlike general-purpose NLG metrics, PatentScore reflects patent-specific constraints and document structures, enabling evaluation beyond surface similarity. We evaluate 400 GPT-4o-mini generated Claim 1s and report a Pearson correlation of $r = 0.819$ with expert annotations, outperforming existing NLG metrics. Furthermore, we conduct additional evaluations using open models such as Claude-3.5-Haiku and Gemini-1.5-flash, all of which show strong correlations with expert judgments, confirming the robustness and generalizability of our framework.</div></div><div class="mb-8"><h2 class="text-xl font-semibold mb-1">Patent-CR: A Dataset for Patent Claim Revision</h2><div class="text-sm text-gray-600 mb-1"><b>Authors:</b> <!-- -->Lekang Jiang, Pascal A Scherz, Stephan Goetz</div><div class="text-sm text-gray-600 mb-1"><b>Published:</b> <!-- -->2024-12-03T16:43:42Z</div><div class="text-sm text-gray-600 mb-2"><b>Link:</b> <a href="http://arxiv.org/abs/2412.02549v2" class="text-blue-600 hover:underline" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2412.02549v2</a></div><div class="whitespace-pre-line">This paper presents Patent-CR, the first dataset created for the patent claim revision task in English. It includes both initial patent applications rejected by patent examiners and the final granted versions. Unlike normal text revision tasks that predominantly focus on enhancing sentence quality, such as grammar correction and coherence improvement, patent claim revision aims at ensuring the claims meet stringent legal criteria. These criteria are beyond novelty and inventiveness, including clarity of scope, technical accuracy, language precision, and legal robustness. We assess various large language models (LLMs) through professional human evaluation, including general LLMs with different sizes and architectures, text revision models, and domain-specific models. Our results indicate that LLMs often bring ineffective edits that deviate from the target revisions. In addition, domain-specific models and the method of fine-tuning show promising results. Notably, GPT-4 outperforms other tested LLMs, but further revisions are still necessary to reach the examination standard. Furthermore, we demonstrate the inconsistency between automated and human evaluation results, suggesting that GPT-4-based automated evaluation has the highest correlation with human judgment. This dataset, along with our preliminary empirical research, offers invaluable insights for further exploration in patent claim revision.</div></div><div class="mb-8"><h2 class="text-xl font-semibold mb-1">Can Large Language Models Generate High-quality Patent Claims?</h2><div class="text-sm text-gray-600 mb-1"><b>Authors:</b> <!-- -->Lekang Jiang, Caiqi Zhang, Pascal A Scherz, Stephan Goetz</div><div class="text-sm text-gray-600 mb-1"><b>Published:</b> <!-- -->2024-06-27T18:07:40Z</div><div class="text-sm text-gray-600 mb-2"><b>Link:</b> <a href="http://arxiv.org/abs/2406.19465v3" class="text-blue-600 hover:underline" target="_blank" rel="noopener noreferrer">http://arxiv.org/abs/2406.19465v3</a></div><div class="whitespace-pre-line">Large language models (LLMs) have shown exceptional performance across various text generation tasks but remain under-explored in the patent domain, which offers highly structured and precise language. This paper constructs a dataset to investigate the performance of current LLMs in patent claim generation. Our results demonstrate that generating claims based on patent descriptions outperforms previous research relying on abstracts. Interestingly, current patent-specific LLMs perform much worse than state-of-the-art general LLMs, highlighting the necessity for future research on in-domain LLMs. We also find that LLMs can produce high-quality first independent claims, but their performances markedly decrease for subsequent dependent claims. Moreover, fine-tuning can enhance the completeness of inventions&#x27; features, conceptual clarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the best performance in comprehensive human evaluations by patent experts, with better feature coverage, conceptual clarity, and technical coherence. Despite these capabilities, comprehensive revision and modification are still necessary to pass rigorous patent scrutiny and ensure legal robustness.</div></div></div><div class="mt-8"><a class="text-gray-700 hover:underline" href="/daily-papers">← Back to Today</a></div></main><!--$--><!--/$--></main><script src="/daily-papers/_next/static/chunks/webpack-fc8e233cfa9e6f69.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[6874,[\"874\",\"static/chunks/874-437a265a67d6cfee.js\",\"408\",\"static/chunks/app/archive/%5Bdate%5D/page-a387a16caeea677c.js\"],\"\"]\n3:I[3063,[\"874\",\"static/chunks/874-437a265a67d6cfee.js\",\"63\",\"static/chunks/63-6e23fca05d5bdafc.js\",\"177\",\"static/chunks/app/layout-9d71c8e95472ad59.js\"],\"Image\"]\n4:I[5838,[\"874\",\"static/chunks/874-437a265a67d6cfee.js\",\"63\",\"static/chunks/63-6e23fca05d5bdafc.js\",\"177\",\"static/chunks/app/layout-9d71c8e95472ad59.js\"],\"default\"]\n5:I[7555,[],\"\"]\n6:I[1295,[],\"\"]\n8:I[9665,[],\"OutletBoundary\"]\na:I[4911,[],\"AsyncMetadataOutlet\"]\nc:I[9665,[],\"ViewportBoundary\"]\ne:I[9665,[],\"MetadataBoundary\"]\nf:\"$Sreact.suspense\"\n11:I[8393,[],\"\"]\n:HL[\"/daily-papers/_next/static/css/a760a3164bba60fd.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"--xLLSGpKpbeK61bOsBjx\",\"p\":\"/daily-papers\",\"c\":[\"\",\"archive\",\"2025-05-26\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"archive\",{\"children\":[[\"date\",\"2025-05-26\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/daily-papers/_next/static/css/a760a3164bba60fd.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"bg-zinc-50 dark:bg-zinc-950 text-zinc-900 dark:text-zinc-100 min-h-screen\",\"children\":[[\"$\",\"header\",null,{\"className\":\"w-full py-4 border-b border-zinc-200 dark:border-zinc-800 mb-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-6xl mx-auto px-4 flex items-center justify-between\",\"children\":[[\"$\",\"$L2\",null,{\"href\":\"/\",\"className\":\"flex items-center gap-3 hover:opacity-80 transition-opacity\",\"children\":[[\"$\",\"$L3\",null,{\"src\":\"/daily-papers/favicon.png\",\"alt\":\"favicon\",\"width\":28,\"height\":28,\"className\":\"rounded\"}],[\"$\",\"span\",null,{\"className\":\"font-bold text-lg tracking-tight\",\"children\":\"Daily Papers\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-4\",\"children\":[\"$\",\"$L4\",null,{}]}]]}]}],[\"$\",\"main\",null,{\"children\":[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]]}]}]]}],{\"children\":[\"archive\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"date\",\"2025-05-26\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L5\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L6\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L7\",null,[\"$\",\"$L8\",null,{\"children\":[\"$L9\",[\"$\",\"$La\",null,{\"promise\":\"$@b\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$Lc\",null,{\"children\":\"$Ld\"}],null],[\"$\",\"$Le\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$f\",null,{\"fallback\":null,\"children\":\"$L10\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"12:T493,"])</script><script>self.__next_f.push([1,"Patent similarity evaluation plays a critical role in intellectual property analysis. However, existing methods often overlook the intricate structure of patent documents, which integrate technical specifications, legal boundaries, and application contexts. We introduce PatentMind, a novel framework for patent similarity assessment based on a Multi-Aspect Reasoning Graph (MARG). PatentMind decomposes patents into three core dimensions: technical feature, application domain, and claim scope, to compute dimension-specific similarity scores. These scores are dynamically weighted through a four-stage reasoning process which integrates contextual signals to emulate expert-level judgment. To support evaluation, we construct PatentSimBench, a human-annotated benchmark comprising 500 patent pairs. Experimental results demonstrate that PatentMind achieves a strong correlation ($r=0.938$) with expert annotations, significantly outperforming embedding-based models and advanced prompt engineering methods.These results highlight the effectiveness of modular reasoning frameworks in overcoming key limitations of embedding-based methods for analyzing patent similarity."])</script><script>self.__next_f.push([1,"13:T5be,"])</script><script>self.__next_f.push([1,"Natural language generation (NLG) metrics play a central role in evaluating generated texts, but are not well suited for the structural and legal characteristics of patent documents. Large language models (LLMs) offer strong potential in automating patent generation, yet research on evaluating LLM-generated patents remains limited, especially in evaluating the generation quality of patent claims, which are central to defining the scope of protection. Effective claim evaluation requires addressing legal validity, technical accuracy, and structural compliance. To address this gap, we introduce PatentScore, a multi-dimensional evaluation framework for assessing LLM-generated patent claims. PatentScore incorporates: (1) hierarchical decomposition for claim analysis; (2) domain-specific validation patterns based on legal and technical standards; and (3) scoring across structural, semantic, and legal dimensions. Unlike general-purpose NLG metrics, PatentScore reflects patent-specific constraints and document structures, enabling evaluation beyond surface similarity. We evaluate 400 GPT-4o-mini generated Claim 1s and report a Pearson correlation of $r = 0.819$ with expert annotations, outperforming existing NLG metrics. Furthermore, we conduct additional evaluations using open models such as Claude-3.5-Haiku and Gemini-1.5-flash, all of which show strong correlations with expert judgments, confirming the robustness and generalizability of our framework."])</script><script>self.__next_f.push([1,"7:[\"$\",\"main\",null,{\"className\":\"max-w-6xl mx-auto py-10 px-4\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-2xl font-extrabold mb-8 tracking-tight text-center\",\"children\":[\"Papers for \",\"2025-05-26\"]}],[\"$\",\"div\",null,{\"className\":\"prose dark:prose-invert border rounded p-4 bg-white dark:bg-zinc-900 mb-8\",\"children\":[[\"$\",\"div\",\"0\",{\"className\":\"mb-8\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mb-1\",\"children\":\"PatentMind: A Multi-Aspect Reasoning Graph for Patent Similarity\\n  Evaluation\"}],[\"$\",\"div\",null,{\"className\":\"text-sm text-gray-600 mb-1\",\"children\":[[\"$\",\"b\",null,{\"children\":\"Authors:\"}],\" \",\"Yongmin Yoo, Qiongkai Xu, Longbing Cao\"]}],[\"$\",\"div\",null,{\"className\":\"text-sm text-gray-600 mb-1\",\"children\":[[\"$\",\"b\",null,{\"children\":\"Published:\"}],\" \",\"2025-05-25T22:28:27Z\"]}],[\"$\",\"div\",null,{\"className\":\"text-sm text-gray-600 mb-2\",\"children\":[[\"$\",\"b\",null,{\"children\":\"Link:\"}],\" \",[\"$\",\"a\",null,{\"href\":\"http://arxiv.org/abs/2505.19347v1\",\"className\":\"text-blue-600 hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"http://arxiv.org/abs/2505.19347v1\"}]]}],[\"$\",\"div\",null,{\"className\":\"whitespace-pre-line\",\"children\":\"$12\"}]]}],[\"$\",\"div\",\"1\",{\"className\":\"mb-8\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mb-1\",\"children\":\"PatentScore: Multi-dimensional Evaluation of LLM-Generated Patent Claims\"}],[\"$\",\"div\",null,{\"className\":\"text-sm text-gray-600 mb-1\",\"children\":[[\"$\",\"b\",null,{\"children\":\"Authors:\"}],\" \",\"Yongmin Yoo, Qiongkai Xu, Longbing Cao\"]}],[\"$\",\"div\",null,{\"className\":\"text-sm text-gray-600 mb-1\",\"children\":[[\"$\",\"b\",null,{\"children\":\"Published:\"}],\" \",\"2025-05-25T22:20:11Z\"]}],[\"$\",\"div\",null,{\"className\":\"text-sm text-gray-600 mb-2\",\"children\":[[\"$\",\"b\",null,{\"children\":\"Link:\"}],\" \",[\"$\",\"a\",null,{\"href\":\"http://arxiv.org/abs/2505.19345v1\",\"className\":\"text-blue-600 hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"http://arxiv.org/abs/2505.19345v1\"}]]}],[\"$\",\"div\",null,{\"className\":\"whitespace-pre-line\",\"children\":\"$13\"}]]}],\"$L14\",\"$L15\"]}],\"$L16\"]}]\n"])</script><script>self.__next_f.push([1,"17:T58c,"])</script><script>self.__next_f.push([1,"This paper presents Patent-CR, the first dataset created for the patent claim revision task in English. It includes both initial patent applications rejected by patent examiners and the final granted versions. Unlike normal text revision tasks that predominantly focus on enhancing sentence quality, such as grammar correction and coherence improvement, patent claim revision aims at ensuring the claims meet stringent legal criteria. These criteria are beyond novelty and inventiveness, including clarity of scope, technical accuracy, language precision, and legal robustness. We assess various large language models (LLMs) through professional human evaluation, including general LLMs with different sizes and architectures, text revision models, and domain-specific models. Our results indicate that LLMs often bring ineffective edits that deviate from the target revisions. In addition, domain-specific models and the method of fine-tuning show promising results. Notably, GPT-4 outperforms other tested LLMs, but further revisions are still necessary to reach the examination standard. Furthermore, we demonstrate the inconsistency between automated and human evaluation results, suggesting that GPT-4-based automated evaluation has the highest correlation with human judgment. This dataset, along with our preliminary empirical research, offers invaluable insights for further exploration in patent claim revision."])</script><script>self.__next_f.push([1,"14:[\"$\",\"div\",\"2\",{\"className\":\"mb-8\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mb-1\",\"children\":\"Patent-CR: A Dataset for Patent Claim Revision\"}],[\"$\",\"div\",null,{\"className\":\"text-sm text-gray-600 mb-1\",\"children\":[[\"$\",\"b\",null,{\"children\":\"Authors:\"}],\" \",\"Lekang Jiang, Pascal A Scherz, Stephan Goetz\"]}],[\"$\",\"div\",null,{\"className\":\"text-sm text-gray-600 mb-1\",\"children\":[[\"$\",\"b\",null,{\"children\":\"Published:\"}],\" \",\"2024-12-03T16:43:42Z\"]}],[\"$\",\"div\",null,{\"className\":\"text-sm text-gray-600 mb-2\",\"children\":[[\"$\",\"b\",null,{\"children\":\"Link:\"}],\" \",[\"$\",\"a\",null,{\"href\":\"http://arxiv.org/abs/2412.02549v2\",\"className\":\"text-blue-600 hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"http://arxiv.org/abs/2412.02549v2\"}]]}],[\"$\",\"div\",null,{\"className\":\"whitespace-pre-line\",\"children\":\"$17\"}]]}]\n"])</script><script>self.__next_f.push([1,"18:T4b8,"])</script><script>self.__next_f.push([1,"Large language models (LLMs) have shown exceptional performance across various text generation tasks but remain under-explored in the patent domain, which offers highly structured and precise language. This paper constructs a dataset to investigate the performance of current LLMs in patent claim generation. Our results demonstrate that generating claims based on patent descriptions outperforms previous research relying on abstracts. Interestingly, current patent-specific LLMs perform much worse than state-of-the-art general LLMs, highlighting the necessity for future research on in-domain LLMs. We also find that LLMs can produce high-quality first independent claims, but their performances markedly decrease for subsequent dependent claims. Moreover, fine-tuning can enhance the completeness of inventions' features, conceptual clarity, and feature linkage. Among the tested LLMs, GPT-4 demonstrates the best performance in comprehensive human evaluations by patent experts, with better feature coverage, conceptual clarity, and technical coherence. Despite these capabilities, comprehensive revision and modification are still necessary to pass rigorous patent scrutiny and ensure legal robustness."])</script><script>self.__next_f.push([1,"15:[\"$\",\"div\",\"3\",{\"className\":\"mb-8\",\"children\":[[\"$\",\"h2\",null,{\"className\":\"text-xl font-semibold mb-1\",\"children\":\"Can Large Language Models Generate High-quality Patent Claims?\"}],[\"$\",\"div\",null,{\"className\":\"text-sm text-gray-600 mb-1\",\"children\":[[\"$\",\"b\",null,{\"children\":\"Authors:\"}],\" \",\"Lekang Jiang, Caiqi Zhang, Pascal A Scherz, Stephan Goetz\"]}],[\"$\",\"div\",null,{\"className\":\"text-sm text-gray-600 mb-1\",\"children\":[[\"$\",\"b\",null,{\"children\":\"Published:\"}],\" \",\"2024-06-27T18:07:40Z\"]}],[\"$\",\"div\",null,{\"className\":\"text-sm text-gray-600 mb-2\",\"children\":[[\"$\",\"b\",null,{\"children\":\"Link:\"}],\" \",[\"$\",\"a\",null,{\"href\":\"http://arxiv.org/abs/2406.19465v3\",\"className\":\"text-blue-600 hover:underline\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"children\":\"http://arxiv.org/abs/2406.19465v3\"}]]}],[\"$\",\"div\",null,{\"className\":\"whitespace-pre-line\",\"children\":\"$18\"}]]}]\n"])</script><script>self.__next_f.push([1,"16:[\"$\",\"div\",null,{\"className\":\"mt-8\",\"children\":[\"$\",\"$L2\",null,{\"href\":\"/\",\"className\":\"text-gray-700 hover:underline\",\"children\":\"← Back to Today\"}]}]\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n9:null\n"])</script><script>self.__next_f.push([1,"19:I[8175,[],\"IconMark\"]\nb:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Daily Papers\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Stay updated with the latest research papers\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/daily-papers/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"1024x1024\"}],[\"$\",\"$L19\",\"3\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"10:\"$b:metadata\"\n"])</script></body></html>